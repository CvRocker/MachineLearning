线性回归有两个主要的函数需要弄明白
假设训练数据集为(x,y),x为特征变量，y为对应的实数值

1.假设函数（h(x)）
h(x)是自变量x的函数

2.损失函数（J）


平方误差函数的使用意图：
求最小的参数是的损失函数J最小

使用梯度下降法求解最优（局部最优）的使J最小的参数
其中alpha是学习率，表示每次迭代的歩长。

如果alpha选择的过小，在每一步的迭代中，下降的距离就非常小，因此在最小化J的时候就需要特别多的步。
如果alpha选择过大，在迭代过程中，可能会越过最小点，甚至不会收敛。

即是固定alpha（learning rate）的值，梯度下降法也能收敛到局部最小点。
在迭代的过程中，梯度下降算法会随着每部的迭代，其求导的部分会越来越小，因此后一次迭代相比前一次迭代所减少的值会越来越小，因此没有必要随着时间的推移降低alpha的值
